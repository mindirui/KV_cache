import torch
from tqdm import tqdm
import json
from datetime import datetime
from pathlib import Path
from PIL import Image
import numpy as np
from hook_generator import create_hook_manager
import einops
from models import util as util 
import torch.nn.functional as F
import os
from models.new_processor import (
    AttentionCache, 
    pipeline_in_caching_mode, 
    pipeline_in_injection_mode
)
from models.new_processor import backup_original_processors, restore_original_processors
from util import get_averaged_noise



# ======================================================================
# 1. è¾…åŠ©å‡½æ•° (Optimized Helper Functions)
# ======================================================================

def setup_output_directory(config: dict) -> Path:
    """æ ¹æ®é…ç½®åˆ›å»ºä¸€ä¸ªç‹¬ç‰¹çš„ã€å¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•ï¼Œå¹¶è¿”å›å…¶è·¯å¾„ã€‚"""
    experiment_name = config.get('name', 'unnamed_experiment').replace(' ', '_')
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    run_dir = Path(config.get('root_output_dir', 'outputs')) / f"{experiment_name}_{timestamp}"
    
    # åŒæ—¶åˆ›å»ºç”¨äºä¿å­˜æ‹¼æ¥å¯¹æ¯”å›¾çš„ç›®å½•
    (run_dir / "comparison_images").mkdir(parents=True, exist_ok=True)
    
    print(f"âœ”ï¸ ç»“æœå°†ä¿å­˜åœ¨: {run_dir}")
    return run_dir

def log_parameters(config: dict, run_dir: Path):
    """
    ã€ä¼˜åŒ–ç‚¹1: è‡ªåŠ¨è®°å½•å‚æ•°ã€‘
    è‡ªåŠ¨å°†é…ç½®å­—å…¸ä¸­çš„å…³é”®ä¿¡æ¯åºåˆ—åŒ–ä¸º JSONï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šã€‚
    """
    log_path = run_dir / "run_logs.json"
    
    # å‡†å¤‡è¦è®°å½•çš„é…ç½®ä¿¡æ¯ï¼Œå¯ä»¥æ’é™¤æ‰åºå¤§çš„å¯¹è±¡
    config_to_log = {}
    for key, value in config.items():
        if key not in ['pipeline', 'dataloader']: # æ’é™¤æ‰æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡
            config_to_log[key] = value

    # ç‰¹åˆ«å¤„ç† hooksï¼Œåªè®°å½•å®ƒä»¬çš„æè¿°ä¿¡æ¯
    config_to_log['hooks_applied'] = [h.description() for h in config.get('hooks_to_apply', [])]

    log_data = {
        "run_configuration": config_to_log,
        "results": [], # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    }
    with open(log_path, 'w', encoding='utf-8') as f:
        # ä½¿ç”¨ json.dump æ¥æ ¼å¼åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿é˜…è¯»
        json.dump(log_data, f, indent=4, default=str) # default=str é˜²æ­¢éƒ¨åˆ†ç±»å‹æ— æ³•åºåˆ—åŒ–
    print(f"âœ”ï¸ é…ç½®æ–‡ä»¶å·²è‡ªåŠ¨è®°å½•åˆ°: {log_path}")

def evaluate_single_view(generated_view_np: np.ndarray, gt_view_tensor: torch.Tensor) -> dict:
    """
    ã€å·²å¡«å……ã€‘å¤„ç†å•ä¸ªè§†è§’çš„é¢„å¤„ç†å’Œè¯„ä¼°ã€‚
    è¿™ä¸ªå‡½æ•°ç°åœ¨åŒ…å«äº†æ‚¨ `preprocess_and_eval` å†…éƒ¨å¾ªç¯çš„æ ¸å¿ƒé€»è¾‘ã€‚
    """
    # 1. é¢„å¤„ç†ï¼šå°† PyTorch Tensor æ ¼å¼çš„çœŸå€¼å›¾è½¬æ¢ä¸º NumPy uint8 æ ¼å¼
    #    è¾“å…¥ gt_view_tensor shape: (3, H, W)
    gt_np_uint8 = (gt_view_tensor.permute(1, 2, 0).cpu().numpy() * 255).clip(0, 255).astype(np.uint8)
    
    # 2. é¢„å¤„ç†ï¼šå°† NumPy float æ ¼å¼çš„ç”Ÿæˆå›¾è½¬æ¢ä¸º NumPy uint8 æ ¼å¼
    #    è¾“å…¥ generated_view_np shape: (H, W, 3)
    pred_np_uint8 = (generated_view_np * 255).clip(0, 255).astype(np.uint8)

    # 3. è°ƒç”¨åº•å±‚çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
    loss, lpips, ssim, psnr = util.calc_2D_metrics(pred_np_uint8, gt_np_uint8)

    # 4. å°†ç»“æœæ‰“åŒ…æˆæˆ‘ä»¬æ¡†æ¶éœ€è¦çš„å­—å…¸æ ¼å¼
    return {
        "loss": loss,
        "lpips": lpips,
        "ssim": ssim,
        "psnr": psnr,
    }

def parse_path_info(full_path: str) -> tuple[str, str]:
    """
    ã€V2 ç‰ˆã€‘ä»ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„ä¸­è§£æå‡ºã€ç‰©ä½“åã€‘å’Œã€æ–‡ä»¶æ ‡è¯†ç¬¦ã€‘ã€‚

    Args:
        full_path (str): å®Œæ•´çš„æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²ã€‚

    Returns:
        tuple[str, str]: ä¸€ä¸ªåŒ…å« (ç‰©ä½“å, æ–‡ä»¶æ ‡è¯†ç¬¦) çš„å…ƒç»„ã€‚
                         ä¾‹å¦‚ï¼š('Circo_Fish_Toothbrush_Holder_14995988', '1_5.0_1.8')
    """
    if not full_path or full_path == "N/A":
        return "N/A", "N/A"
    
    try:
        p = Path(full_path)
        # p.stem ä¼šè·å–æ–‡ä»¶åä½†å»é™¤æœ€åçš„æ‰©å±•åï¼ˆ.pngï¼‰
        identifier = p.stem
        object_name = p.parent.parent.name
        return object_name, identifier
    except Exception as e:
        print(f"è­¦å‘Š: è§£æè·¯å¾„ '{full_path}' å¤±è´¥: {e}")
        return "parsing_error", Path(full_path).stem


def evaluate_and_log_batch(
    generated_images_np: np.ndarray, 
    ground_truth_tensor: torch.Tensor,
    batch_info: dict,
    batch_idx: int,
    run_dir: Path
) -> list[dict]:
    """
    ã€V6 ç‰ˆ - æœ€ç»ˆç‰ˆã€‘ç”ŸæˆåŒ…å«ã€æ‰¹æ¬¡æ¦‚è§ˆã€‘å’Œã€è§†è§’è¯¦æƒ…ã€‘çš„å±‚æ¬¡åŒ–æ—¥å¿—ã€‚
    """
    # 1. é¢„å¤„ç† (ä¸å˜)
    num_views = generated_images_np.shape[0]
    gt_squeezed = ground_truth_tensor.squeeze(0)
    gt_resized = F.interpolate(
        gt_squeezed, 
        size=(generated_images_np.shape[1], generated_images_np.shape[2]), 
        mode='bilinear', 
        align_corners=False
    )

    # 2. è§£æè·¯å¾„ä¿¡æ¯ï¼Œç”¨äºé¡¶å±‚æ¦‚è§ˆ (ä¸å˜)
    input_paths = batch_info.get("input_path", [])
    output_paths = batch_info.get("output_path", [])
    object_name = parse_path_info(input_paths[0][0])[0] if input_paths else "N/A"
    input_identifiers = [parse_path_info(p[0])[1] for p in input_paths]
    output_identifiers = [parse_path_info(p[0])[1] for p in output_paths]
    
    # 3. ã€ä¿ç•™ã€‘å¾ªç¯å¤„ç†æ¯ä¸ªè§†è§’ï¼Œæ„å»ºåŒ…å«å®Œæ•´ç»†èŠ‚çš„åˆ—è¡¨
    per_view_details_list = []
    for view_idx in range(num_views):
        view_metrics = evaluate_single_view(
            generated_view_np=generated_images_np[view_idx], 
            gt_view_tensor=gt_resized[view_idx]
        )
        
        raw_input_path = input_paths[view_idx][0] if view_idx < len(input_paths) else "N/A"
        raw_output_path = output_paths[view_idx][0] if view_idx < len(output_paths) else "N/A"
        
        # æ„å»ºæ¯ä¸ªè§†è§’çš„è¯¦ç»†ä¿¡æ¯å­—å…¸
        view_detail_entry = {
            "view_index": view_idx,
            "input_path_full": raw_input_path,
            "ground_truth_path_full": raw_output_path,
            "metrics": view_metrics
        }
        per_view_details_list.append(view_detail_entry)

    # 4. è®¡ç®—æ‰¹æ¬¡å¹³å‡æŒ‡æ ‡ (ä¸å˜)
    metrics_for_avg = [result['metrics'] for result in per_view_details_list]
    batch_average_metrics = calculate_average_metrics(metrics_for_avg)
    
    # 5. ä¿å­˜å¯¹æ¯”å›¾ (ä¸å˜)
    save_path = run_dir / "comparison_images" / f"compare_{batch_idx:04d}.png"
    util.save_concat_images(gt_resized,generated_images_np , save_path)
    
    # 6. ã€å…³é”®ã€‘æ„å»ºæœ€ç»ˆçš„ã€å±‚æ¬¡åŒ–çš„æ—¥å¿—æ¡ç›®
    log_entry = {
        # --- é¡¶å±‚æ¦‚è§ˆä¿¡æ¯ ---
        "batch_index": batch_idx,
        "path": object_name,
        "input_index": ",".join(input_identifiers),
        "output_index": ",".join(output_identifiers),
        "num_views": num_views,
        "saved_comparison_image": str(save_path),
        "batch_average_metrics": batch_average_metrics,
        
        # --- ä¿ç•™çš„ã€è¯¦ç»†çš„æ¯ä¸ªè§†è§’çš„ä¿¡æ¯ ---
        "per_view_details": per_view_details_list 
    }
    
    update_log_file(run_dir, {"result_item": log_entry})
    
    # 7. è¿”å›ç”¨äºè®¡ç®—å…¨å±€å¹³å‡å€¼çš„æŒ‡æ ‡åˆ—è¡¨ (ä¸å˜)
    return metrics_for_avg

def update_log_file(run_dir: Path, entry: dict):
    """ã€ä¼˜åŒ–ç‚¹3: å®‰å…¨çš„æ—¥å¿—æ›´æ–°ã€‘è¯»å–ã€æ›´æ–°ã€å†™å›JSONï¼Œè€Œéé‡å†™æ•´ä¸ªæ–‡ä»¶ã€‚"""
    log_path = run_dir / "run_logs.json"
    # ä½¿ç”¨æ–‡ä»¶é”å¯ä»¥è¿›ä¸€æ­¥å¢åŠ å¹¶å‘å®‰å…¨æ€§ï¼Œä½†åœ¨è¿™é‡Œæš‚æ—¶ç®€åŒ–
    with open(log_path, 'r+', encoding='utf-8') as f:
        log_data = json.load(f)
        
        if "result_item" in entry:
            log_data["results"].append(entry["result_item"])
        elif "summary" in entry:
            log_data["summary"] = entry["summary"]
            
        f.seek(0)
        json.dump(log_data, f, indent=4, default=str)
        f.truncate()

# ... (calculate_average_metrics å‡½æ•°ä¿æŒä¸å˜) ...
def calculate_average_metrics(all_metrics: list[dict]) -> dict:
    if not all_metrics: return {}
    summary = {}
    metric_keys = all_metrics[0].keys()
    for key in metric_keys:
        # ä½¿ç”¨ np.nanmean å¯ä»¥å®‰å…¨åœ°å¤„ç†å¯èƒ½å­˜åœ¨çš„ NaN å€¼
        summary[key] = float(np.nanmean([m[key] for m in all_metrics]))
    return summary


def prepare_batch_for_pipeline(batch: dict, device: torch.device, weight_dtype: torch.dtype) -> dict:
    """
    æ¥æ”¶åŸå§‹æ‰¹æ¬¡æ•°æ®ï¼Œå‡†å¤‡å¥½ pipeline è°ƒç”¨æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ã€‚
    """
    # å‡è®¾ 'resize_and_normalize_batch_images' æ˜¯ä¸€ä¸ªæ‚¨å·²ç»å®šä¹‰çš„å‡½æ•°
    # from utils import resize_and_normalize_batch_images
    
    # --- å¼€å§‹ç§»æ¤æ‚¨çš„ä»£ç  ---
    input_images = batch["input_images"]
    input_images = resize_and_normalize_batch_images(input_images)
    
    pose_in = batch["input_poses"]
    pose_out = batch["output_poses"]
    
    # è¿™é‡Œçš„ squeeze(0) å‡è®¾ batch_size=1ï¼Œå¦‚æœä¸æ˜¯ï¼Œé€»è¾‘å¯èƒ½éœ€è¦è°ƒæ•´
    pose_in_np = pose_in.squeeze(0).cpu().numpy()
    pose_out_np = pose_out.squeeze(0).cpu().numpy()
    
    pose_in_inv_np = np.linalg.inv(pose_in_np).transpose([0, 2, 1])
    pose_out_inv_np = np.linalg.inv(pose_out_np).transpose([0, 2, 1])
    
    pose_in_inv = torch.from_numpy(pose_in_inv_np).to(device=device, dtype=weight_dtype).unsqueeze(0)
    pose_out_inv = torch.from_numpy(pose_out_inv_np).to(device=device, dtype=weight_dtype).unsqueeze(0)
    pose_in = torch.from_numpy(pose_in_np).to(device=device, dtype=weight_dtype).unsqueeze(0)
    pose_out = torch.from_numpy(pose_out_np).to(device=device, dtype=weight_dtype).unsqueeze(0)
    
    input_images = einops.rearrange(input_images, "b t c h w -> (b t) c h w")
    
    T_in = pose_in.shape[1]
    T_out = pose_out.shape[1]
    
    # ä»åŸå§‹ batch ä¸­è·å–å…¶ä»–æ‰€éœ€æ•°æ®
    gt_images = batch["output_images"]
    # h, w = batch.get("height"), batch.get("width") # å‡è®¾ h, w ä¹Ÿåœ¨è¿™é‡Œ
    # mask = batch.get("prompt_img_mask")
    # mask_config = batch.get("mask_config")
    
    # --- å°†æ‰€æœ‰ pipeline å‚æ•°æ‰“åŒ…æˆä¸€ä¸ªå­—å…¸ ---
    pipeline_args = {
        "input_imgs": input_images,
        "prompt_imgs": input_images, # æ‚¨çš„ä»£ç ä¸­ prompt_imgs å’Œ input_imgs ç›¸åŒ
        "poses": [[pose_out, pose_out_inv], [pose_in, pose_in_inv]],
        "height": 256, # ä½¿ç”¨å›ºå®šå€¼æˆ–ä» batch ä¸­è·å–
        "width": 256,
        "T_in": T_in,
        "T_out": T_out,
        # "prompt_img_mask": mask,
        # "mask_config": mask_config,
    }

    # --- è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤éƒ¨åˆ†çš„å­—å…¸ï¼špipelineå‚æ•°å’ŒçœŸå€¼æ•°æ® ---
    return {
        "pipeline_args": pipeline_args,
        "ground_truth": gt_images
    }

def resize_and_normalize_batch_images(input_images, resolution=256):
    """
    input_images: torch.Tensor of shape (B, V, 3, H, W), values in range [0, 1] or [0, 255]
    Output: resized and normalized images of shape (B, V, 3, resolution, resolution)
    """
    B, V = input_images.shape[:2]

    # å¦‚æœè¾“å…¥æ˜¯ [0, 255]ï¼Œå¯é€‰è¿›è¡Œå½’ä¸€åŒ–åˆ° [0, 1]ï¼ˆè§†æ•°æ®è€Œå®šï¼‰
    if input_images.max() > 1:
        input_images = input_images / 255.0

    # Resize
    resized_images = F.interpolate(
        input_images.view(-1, 3, input_images.shape[-2], input_images.shape[-1]),
        size=(resolution, resolution),
        mode='bilinear',
        align_corners=False
    )

    # Normalize: å½’ä¸€åŒ–åˆ° [-1, 1]ï¼ˆæ¨¡æ‹Ÿ torchvision.transforms.Normalize([0.5], [0.5])ï¼‰
    resized_images = resized_images * 2 - 1

    # æ¢å¤å½¢çŠ¶
    output_images = resized_images.view(B, V, 3, resolution, resolution)
    return output_images

def prepare_data_for_cache(full_pipeline_args, reference_view_index):
        caching_input_imgs = full_pipeline_args["input_imgs"] # æ¡ä»¶æ˜¯å…¨éƒ¨3å¼ è¾“å…¥å›¾
        caching_input_poses_list = full_pipeline_args["poses"][1]
        # reconstruction_target_img = full_pipeline_args["input_imgs"][reference_view_index : reference_view_index + 1, ...]
        reconstruction_target_poses_list = [p[:, reference_view_index : reference_view_index + 1, ...] for p in caching_input_poses_list]  
        caching_pipeline_args = {
            "input_imgs": caching_input_imgs,
            "prompt_imgs": caching_input_imgs, 
            "poses": [reconstruction_target_poses_list, caching_input_poses_list],
            "height": full_pipeline_args["height"],
            "width": full_pipeline_args["width"],
            "T_in": full_pipeline_args["T_in"],
            "T_out": 1,
        }
        return caching_pipeline_args
    
def prepare_noise_reschedule(reconstruction_target_img, 
                             pipeline, 
                             generator, 
                             reference_fidelity, 
                             noise_avg_samples, 
                             total_steps,
                             test = False):
    if reference_fidelity != 0:
        z0 = pipeline.vae.encode(reconstruction_target_img.to(pipeline.device)).latent_dist.sample() * pipeline.vae.config.scaling_factor
        timesteps = pipeline.scheduler.timesteps
        start_index = int(reference_fidelity * total_steps)
        start_timestep = timesteps[start_index]
        noise = get_averaged_noise(z0, generator, noise_avg_samples)
        noisy_latents = pipeline.scheduler.add_noise(z0, noise, start_timestep)
        remaining_timesteps_array = timesteps[start_index:]
        return noisy_latents, remaining_timesteps_array
    else:
        return None, None

# ======================================================================
# 2. æ ¸å¿ƒæ¨ç†å‡½æ•° (Main Inference Function)
# ======================================================================


def run_inference_task(config: dict):
    """ä¸€ä¸ªçº¯å‡€çš„æ¨ç†å‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªé…ç½®å­—å…¸å¹¶æ‰§è¡Œä»å¤´åˆ°å°¾çš„å®Œæ•´æµç¨‹ã€‚"""
    print(f"\nğŸš€ === å¼€å§‹ä»»åŠ¡: {config.get('name', 'Untitled')} ===")
    
    ## --- é˜¶æ®µä¸€: è®¾ç½® ---
    pipeline = config['pipeline']
    dataloader = config['dataloader']
    main_generator = config['generator']
    static_inference_args = config.get('inference_args', {})
    device = pipeline.device
    weight_dtype = torch.float16
    ## parameters for attention cache
    use_cache = config.get("use_cache", True)  ## disable cache to test baseline
    rgs_function = config.get("attention_store", None)  ## func to manipulate the rgs-strength
    reference_view_index = config.get("reference_view_index", 1)  ## choose one reference-image to help guiding generation
    ## parameters for noise_reschedule
    reference_fidelity = config.get("reference_fidelity", 0) ## Reference Image Fidelity. NOTE: A LOWER value means MORE noise and GREATER change from the reference. Range [0, 1]. 0 means starting from pure noise.
    noise_avg_samples  = config.get("noise_avg_samples", 1) ## Number of noise samples to average for a more stable start. Set to 1 to disable averaging.
    total_steps = config.get('inference_args', {}).get('num_inference_steps', 50) ## Total number of inference/denoising steps, defining the scheduler's granularity.
    ## paremeters for debug and visualization
    save_attention_map = config.get("save_attention_map", False) ## [Debug] Whether to save attention maps for analysis.
    save_interlatents = config.get("save_interlatents", False) ## [Debug] Whether to save intermediate latents during the denoising process.
    callback_step = config.get("callback_step", 1) ## [Debug] Frequency of saving intermediate latents.
    test_inference = config.get("test_inference", False) ## [Debug] If True, save no logs and only print mertrics
    ## parameters for output
    sample_metrics = config.get('sample_metrics', None) ## [Debug] If test NinNout baseline outputs, only focus on the first output image to compare with our Nin1out

    
    ## initialize a call_back_handler to restore interlatents
    if save_interlatents:
        from models.util import FinalCallbackHandler
        callback_handler = FinalCallbackHandler(pipe=pipeline, output_dir="cache_interlatent", run_name="")
    else:
        callback_handler = None
    
    ## initialize a cache 
    if use_cache:
        cache = AttentionCache()
    else:
        cache = None
        
    ## initialize output 
    run_dir = setup_output_directory(config)
    if not test_inference:
        log_parameters(config, run_dir)
        all_metrics_across_batches = [] 
        
    
    for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"æ¨ç†ä¸­...")):
        # a. ä¸ºå½“å‰æ‰¹æ¬¡å‡†å¤‡å®Œæ•´çš„è¾“å…¥æ•°æ®
        prepared_data = prepare_batch_for_pipeline(batch, device, weight_dtype)
        full_pipeline_args = prepared_data['pipeline_args']
        item_id = f"batch_{batch_idx:04d}"
        
        # ==========================================================
        # 1. å‡†å¤‡å¹¶æ‰§è¡Œâ€œç¼“å­˜â€é˜¶æ®µ (3-in-1-out é‡å»º)
        # ==========================================================
        # a. ä»å®Œæ•´æ•°æ®ä¸­â€œåˆ‡ç‰‡â€å‡ºç”¨äºé‡å»ºçš„å‚æ•°
        if use_cache:
            cache.clear()
            cache_pipeline_args = prepare_data_for_cache(full_pipeline_args, reference_view_index)
            cache_generator = torch.Generator(device=device).manual_seed(999)
            # store.reset()
            with torch.autocast("cuda", dtype=weight_dtype):
                with torch.no_grad():
                    generated_images_np = pipeline(
                        generator=cache_generator,
                        **cache_pipeline_args,
                        **static_inference_args,
                        cross_attention_kwargs={
                            "attention_cache": cache,
                            "hijack_mode": "caching",
                            "save_attention_map":save_attention_map,
                        },
                        callback = callback_handler,
                        callback_steps = callback_step,
                    ).images[0]


        
        ## use reference_view_index to select the reference image
        if save_attention_map:
            noisy_latents = None
            remaining_timesteps_array = torch.ones(50)
        else:
            reconstruction_target_img = full_pipeline_args["input_imgs"][reference_view_index : reference_view_index + 1, ...]
            noisy_latents, remaining_timesteps_array = prepare_noise_reschedule(reconstruction_target_img, 
                                                                                pipeline, main_generator, 
                                                                                reference_fidelity, 
                                                                                noise_avg_samples,
                                                                                total_steps,
                                                                                )       
        
        with torch.autocast("cuda", dtype=weight_dtype):
            with torch.no_grad():
                generated_images_np = pipeline(
                    latents = noisy_latents,
                    num_inference_steps = len(remaining_timesteps_array) if remaining_timesteps_array !=None else 50,
                    **full_pipeline_args,
                    cross_attention_kwargs={
                        "attention_cache": cache,
                        "hijack_mode": "injecting",
                        'rgs_function': rgs_function,
                        "save_attention_map":save_attention_map,
                        "need_save":bool(save_attention_map)
                        },
                    output_type="numpy",
                    ).images
        if save_attention_map:
            captured_maps = save_attention_map.maps
            output_filename = f"cache_attention.pt"
            torch.save(captured_maps, output_filename)
            save_attention_map.reset()
            
            
        if test_inference:
            util.evaluate_and_save_single(generated_images_np, prepared_data['ground_truth'])
        else: 
            item_metrics = evaluate_and_log_batch(
                generated_images_np, 
                prepared_data['ground_truth'], 
                batch, 
                batch_idx,
                run_dir
            )
            all_metrics_across_batches.extend(item_metrics)

    if not test_inference:
        if all_metrics_across_batches:
            sample_metrics_limit = sample_metrics
            if sample_metrics_limit:
                print(f"æ³¨æ„: æœ€ç»ˆæŒ‡æ ‡ä»…åŸºäºå‰ {sample_metrics_limit} ä¸ªæ ·æœ¬è®¡ç®—ã€‚")
                metrics_to_summarize = all_metrics_across_batches[:,:sample_metrics_limit]
            else:
                metrics_to_summarize = all_metrics_across_batches

        summary_metrics = calculate_average_metrics(metrics_to_summarize)
        print("\n--- ä»»åŠ¡æ€»ç»“ ---")
        summary_str = ", ".join([f"Avg {k.upper()}: {v:.4f}" for k,v in summary_metrics.items()])
        print(summary_str)
        update_log_file(run_dir, {"summary": {"final_metrics": summary_metrics, "notes": f"Metrics calculated on {len(metrics_to_summarize)} samples."}})
        print(f"âœ… === ä»»åŠ¡å®Œæˆ: {config.get('name', 'Untitled')} ===")
        print(f"è¯¦ç»†æ—¥å¿—è¯·è§: {run_dir / 'run_logs.json'}") 
            